import math
import json

import tensorflow as tf
import numpy as np
from numpy import dot
from numpy.linalg import norm

from nncompress.backend.tensorflow_.transformation.pruning_parser import PruningNNParser, NNParser, serialize
from nncompress import backend as M
from group_fisher import make_group_fisher, add_gates, prune_step, compute_positions, flatten

from prep import add_augmentation, change_dtype

# path is generated by assiging 2-bit number.
# (0,0,0) -> a path by skipping all layers.
# (1,1,1) -> a sequential path

import tensorflow as tf
import numpy as np

def get_flops(model, model_inputs) -> float:
        """
        Calculate FLOPS [GFLOPs] for a tf.keras.Model or tf.keras.Sequential model
        in inference mode. It uses tf.compat.v1.profiler under the hood.
        """
        # if not hasattr(model, "model"):
        #     raise wandb.Error("self.model must be set before using this method.")

        if not isinstance(
            model, (tf.keras.models.Sequential, tf.keras.models.Model)
        ):
            raise ValueError(
                "Calculating FLOPS is only supported for "
                "`tf.keras.Model` and `tf.keras.Sequential` instances."
            )

        from tensorflow.python.framework.convert_to_constants import (
            convert_variables_to_constants_v2_as_graph,
        )

        # Compute FLOPs for one sample
        batch_size = 1
        inputs = [
            tf.TensorSpec([batch_size] + list(inp.shape[1:]), inp.dtype)
            for inp in model_inputs
        ]

        # convert tf.keras model into frozen graph to count FLOPs about operations used at inference
        real_model = tf.function(model).get_concrete_function(inputs)
        frozen_func, _ = convert_variables_to_constants_v2_as_graph(real_model)

        # Calculate FLOPs with tf.profiler
        run_meta = tf.compat.v1.RunMetadata()
        opts = (
            tf.compat.v1.profiler.ProfileOptionBuilder(
                tf.compat.v1.profiler.ProfileOptionBuilder().float_operation()
            )
            .with_empty_output()
            .build()
        )

        flops = tf.compat.v1.profiler.profile(
            graph=frozen_func.graph, run_meta=run_meta, cmd="scope", options=opts
        )

        tf.compat.v1.reset_default_graph()

        # convert to GFLOPs
        return (flops.total_float_ops / 1e9)/2
    
    

def decode(encoding_number, group_size):
    decoded = bin(encoding_number)[2:].zfill(group_size)
    print(decoded)

    # edge extraction
    skip_start = None
    path = []
    for k, i in enumerate(decoded):
        #print(k, i, path)
        if i == "0":
            if k == len(decoded)-1:
                if skip_start is None:
                    path.append(k+len(decoded))
                else:
                    path.append((skip_start, k))
                    skip_start = None
            elif skip_start is None:
                skip_start = k

        elif i == "1":
            if skip_start is None:
                path.append(k)
            elif k - skip_start == 1: # single edge path
                path.append((k-1)+len(decoded)) # skip edge is added at the next iteration together.
                path.append(k)
                skip_start = None
            else: # multiple edge path
                path.append((skip_start, k-1))
                path.append(k)
                skip_start = None
        else:
            raise ValueError("decoding error")

    return path

def _traverse(start, nodes, group_size, pset):
    # edge-graph

    stk = [start]
    visited = set()
    visited.add(start)
    while len(stk) > 0:
 
        curr = stk.pop()
        if type(curr) == int:
            end = curr
        else:
            end = curr[1]

        if end == group_size-1 and curr != start:
            return True

        for edge in nodes[curr]["outbounds"]:
            if edge not in visited and edge in pset:
                stk.append(edge)

    return False

def construct_pathset(groups, include):

    psets = []
    for idx, group in enumerate(groups):
        print("idxxxxxxxxxxx ", idx)
        local_inc = include[idx]
        
        paths = []
        for p in local_inc:
            path = decode(p, len(group))
            paths.append(path)

        # merge
        skip_covered = [[] for _ in range(len(group))]
        pset = set()
        psets.append(pset)
        temp = []
        nodes = {}

        print("path", paths)
        exists = set()
        for path in paths:
            for edge in path:
                if type(edge) != tuple: # short edge
                    pset.add(edge)
                    exists.add(edge)
                    if edge > len(group)-1:
                        nodes[edge-len(group)] = {"outbounds":[]}
                        if True not in skip_covered[edge-len(group)]:
                            skip_covered[edge-len(group)].append(True)
                else:
                    temp.append(edge)
                    if edge not in skip_covered[edge[0]]:
                        skip_covered[edge[0]].append(edge)
                    nodes[edge] = {"outbounds":[]}
                    exists.add(edge[1])
           
        print("skip covered", skip_covered)
        # construct conn_graph
        for i in range(len(skip_covered)):
            if i == len(skip_covered)-1:
                continue

            for val in skip_covered[i]:
                if val is True:
                    next_ = i+1
                else:
                    next_ = val[1]+1

                if next_ > len(skip_covered)-1:
                    continue
                assert next_ in exists

                for out in skip_covered[next_]:
                    if out is True:
                        out = next_
                    if val is True:
                        nodes[i]["outbounds"].append(out)
                    else:
                        nodes[val]["outbounds"].append(out)

        temp = sorted(temp, key= lambda x: x[1]-x[0])
        for edge in temp:

            covered_ = False
            for val in skip_covered[edge[0]]:
                if val is True:
                    start = edge[0]
                else:
                    start = val

                    if val[1] > edge[1]:
                        continue

                if start == edge:
                    continue

                if _traverse(start, nodes, len(group), pset):
                    covered_ = True
                    break

            if not covered_:
                pset.add(edge)

    return psets

def modify_inbound(layer, add, remove):
    
    assert len(layer["inbound_nodes"]) == 1
    inbound = layer["inbound_nodes"]
    to_remove = []
    for flow in inbound:
        for ib in flow:
            assert type(ib[0]) == str
            if ib[0] in remove:
                to_remove.append(ib)
    for r in to_remove:
        inbound[0].remove(r) # single flow
   
    for a in add:
        a = [a, 0, 0, {}]
        inbound[0].append(a) # single flow

def range_search(parser, start, end):
    ret = []
    for n in parser._graph.nodes:
        if start < parser.torder[n] and parser.torder[n] < end:
            ret.append(n)
    return ret

def replace_input(target_dict, src, dst):
    inbound = target_dict["inbound_nodes"]
    for flow in inbound:
        for ib in flow:
            if ib[0] == src:
                ib[0] = dst

def get_add_inputs(group, edge, layer_dict, parser):
    ret = []
    add = group[edge][0]
    inbound = layer_dict[add]["inbound_nodes"]
    for flow in inbound:
        for ib in flow:
            ret.append(ib[0])

    if parser.torder[ret[0]] > parser.torder[ret[1]]:
        ret.reverse()
    return ret


def make_train_model(model, groups, cmodel_groups, scale=0.1, teacher_freeze=True):

    tmodel = M.add_prefix(model, "t_", not_change_input=True)

    outputs = []
    outputs.extend(tmodel.outputs)
    output_map = []

    _inputs = [input_ for input_ in tmodel.inputs]
    _outputs = [output_ for output_ in tmodel.outputs]

    if teacher_freeze:
        for layer in tmodel.layers:
            layer.trainable = False
    else:
        for layer in tmodel.layers:
            layer.trainable = True

    output_idx = {}
    cascade = False
    t_inputs = None
    t_outputs = None
    for gidx, (g, cmodel_group) in enumerate(zip(groups, cmodel_groups)):

        bottom = g[0][2][0][0]
        if g[0][2][0][1] > g[0][2][1][1]:
            bottom = g[0][2][1][0]
        top = g[-1][0]

        if t_inputs is None or (not cascade) or tmodel.get_layer("t_"+bottom).output.shape != t_inputs[0].shape:
            t_inputs = [tmodel.get_layer("t_"+bottom).output]
            t_outputs = [tmodel.get_layer("t_"+top).output]

        cascade = True
        for cid, cmodel in enumerate(cmodel_group):

            if len(cmodel.trainable_weights) == 0:
                print("skipped!!!")
                continue
            cascade = False

            if type(cmodel.inputs) == list:
                out_ = cmodel(t_inputs)
            else:
                out_ = cmodel(t_inputs[0])

            for l in cmodel.layers:
                l.trainable = True

            if type(out_) != list:
                t_out = t_outputs[0]

                if t_out.name not in output_idx:
                    outputs.append(t_out)
                    output_idx[t_out.name] = len(outputs)-1

                if out_.name not in output_idx:
                    outputs.append(out_)
                    output_idx[out_.name] = len(outputs)-1

                output_map.append((t_out, out_))
            else:
                for t_out, a_out in zip(t_outputs, out_):

                    if t_out.name not in output_idx:
                        outputs.append(t_out)
                        output_idx[t_out.name] = len(outputs)-1

                    if a_out.name not in output_idx:
                        outputs.append(a_out)
                        output_idx[a_out.name] = len(outputs)-1

                    output_map.append((t_out, a_out))

    house = tf.keras.Model(tmodel.inputs, outputs) # for test

    for (t, s) in output_map:
        t = tf.cast(house.outputs[output_idx[t.name]], tf.float32)
        s = tf.cast(house.outputs[output_idx[s.name]], tf.float32)
        house.add_loss(tf.reduce_mean(tf.keras.losses.mean_squared_error(t, s)*scale))

    #for (t, s) in output_map:
    #    house.add_loss(tf.reduce_mean(tf.keras.losses.mean_squared_error(t, s)*scale))
    return house




   
def psets2model(model, groups, psets, parser, custom_objects=None):

    # psets: [{0, 1}, {0, 1}, {(0, 1), 1, 2}, {(0, 1), 1, 2}, {0, 1, 2, 3, 5}]
    # groups: [[('block2b_add', 51, [('block2b_drop', 50), ('block2a_project_bn', 34)])], [('block3b_add', 83, [('block3b_drop', 82), ('block3a_project_bn', 66)])], [('block4b_add', 115, [('block4b_drop', 114), ('block4a_project_bn', 98)]), ('block4c_add', 132, [('block4c_drop', 131), ('block4b_add', 115)])], [('block5b_add', 164, [('block5b_drop', 163), ('block5a_project_bn', 147)]), ('block5c_add', 181, [('block5c_drop', 180), ('block5b_add', 164)])], [('block6b_add', 213, [('block6b_drop', 212), ('block6a_project_bn', 196)]), ('block6c_add', 230, [('block6c_drop', 229), ('block6b_add', 213)]), ('block6d_add', 247, [('block6d_drop', 246), ('block6c_add', 230)])]]

    conn_to = {}
    for g in groups:
        for item in g:
            add_name = item[0]
            node = parser.get_nodes([add_name])[0]
            neighbors = parser._graph.out_edges(node[0], data=True)

            conn_to[add_name] = []
            for n in neighbors:
                conn_to[add_name].append(n[1])

    model_dict = json.loads(model.to_json())
    layer_dict = {}
    for layer in model_dict["config"]["layers"]:
        layer_dict[layer["name"]] = layer

    removed_layers = []
    alive_layers = []
    for group, pset in zip(groups, psets):
        for idx in range(len(group)):
            add_name = group[idx][0]
            body = False
            skip = False
            jumping = []
            for p in pset:
                if type(p) != tuple:
                    if idx == p:
                        body = True
                    elif p > len(group)-1 and idx == p-len(group):
                        skip = True
                else:
                    if p[-1] == idx:
                        jumping.append(p)

            a = None
            b = None
            for ib in layer_dict[add_name]["inbound_nodes"]:
                a = ib[0][0]
                b = ib[1][0]

                if parser.torder[a] > parser.torder[b]:
                    temp = a
                    a = b
                    b = temp

                break # assume unique flow.

            old_a = group[idx][2][0][0]
            if old_a == b:
                old_a = group[idx][2][1][0]

            if int(body) + int(skip) + len(jumping) > 1: # not erase add
                add_layer = layer_dict[add_name]
                remove = []
                if not body:
                    remove.append(b)
                    removed_layers += range_search(parser, parser.torder[old_a], parser.torder[add_name])
                else:
                    alive_layers += range_search(parser, parser.torder[old_a], parser.torder[add_name])

                    #if a in [x[0] for x in group[idx][2]]:
                    #    alive_layers += range_search(parser, parser.torder[a]-1, parser.torder[add_name])
                if not skip:
                    remove.append(a)

                add = []
                for jump in jumping:
                    sour = jump[0]
                    ret = get_add_inputs(group, sour, layer_dict, parser) # return sour's inputs
                    add.append(ret[0])

                modify_inbound(add_layer, add, remove)

            elif int(body) + int(skip) + len(jumping) == 1:
                removed_layers.append(add_name)

                target_dicts = [ layer_dict[conn] for conn in conn_to[add_name] ]
                if body:
                    for target_dict in target_dicts:
                        replace_input(target_dict, add_name, b)
                    alive_layers += range_search(parser, parser.torder[old_a], parser.torder[add_name])
                elif skip:
                    for target_dict in target_dicts:
                        replace_input(target_dict, add_name, a)
                    removed_layers += range_search(parser, parser.torder[old_a], parser.torder[add_name])
                else: #jumping
                    jump = jumping[0]
                    sour = jump[0]
                    ret = get_add_inputs(group, sour, layer_dict, parser) # return sour's inputs
                    for target_dict in target_dicts:
                        replace_input(target_dict, add_name, ret[0])
                    removed_layers += range_search(parser, parser.torder[old_a], parser.torder[add_name])

            else:
                removed_layers.append(add_name)
                removed_layers += range_search(parser, parser.torder[old_a], parser.torder[add_name])

                target_dicts = [ layer_dict[conn] for conn in conn_to[add_name] ]

                ret = get_add_inputs(group, 0, layer_dict, parser) # return sour's inputs
                for target_dict in target_dicts:
                    replace_input(target_dict, add_name, ret[0])

    removed = list()
    for r in removed_layers:
        if r in alive_layers:
            continue
        if layer_dict[r] in model_dict["config"]["layers"] :
            model_dict["config"]["layers"].remove(layer_dict[r])
        removed.append(layer_dict[r])

    if model_dict["config"]["output_layers"][0][0] in removed_layers:
        assert len(model_dict["config"]["layers"]) >= 1
        input_layer = model_dict["config"]["layers"][0]["config"]["name"]
        layer = tf.keras.layers.Lambda(lambda x: x, name="temp_output")
        dict_ = serialize(layer)
        dict_["inbound_nodes"].append(
            [[input_layer, 0, 0, {}]]
        )
        model_dict["config"]["layers"].append(dict_)
        model_dict["config"]["output_layers"][0][0] = dict_["config"]["name"]

    model_json = json.dumps(model_dict)
    cmodel = tf.keras.models.model_from_json(model_json, custom_objects=custom_objects)

    for layer in cmodel.layers:
        try:
            layer.set_weights(model.get_layer(layer.name).get_weights())
        except Exception as e:
            pass # ignore
    return cmodel


def renormalize(model, cmodel, data): 
    pass


def eval_(model, cmodel, data):

    avg_err = 0
    cnt = 0
    for x in data:

        if type(x) != dict:
            if type(model.input) == list and len(model.input[0].shape) != len(x.shape):
                input_ = np.array([x], dtype=np.float32)
            elif type(model.input) == dict and len(model.input["iamge"].shape) != len(x.shape):
                input_ = np.array([x], dtype=np.float32)
            elif type(model.input) != list and len(model.input.shape) != len(x.shape):
                input_ = np.array([x], dtype=np.float32)
            else:
                input_ = tf.cast(x, dtype=tf.float32)
        else:

            input_ = x

        y1 = model(input_)
        y2 = cmodel(input_)

        assert not np.any(np.isinf(y1))
        assert not np.any(np.isinf(y2))

        y1 = tf.cast(y1, dtype=tf.float32)
        y2 = tf.cast(y2, dtype=tf.float32)

        avg_err += np.average(np.linalg.norm(y2-y1, ord=2, axis=-1)) # channel-wise normalization
        cnt += 1
    """
    avg_err = 0
    cnt = 0
    for x1,x2 in zip(data[:len(data)//2], data[len(data)//2:]):
        y11 = model(x1)
        y12 = model(x2)

        y21 = cmodel(x1)
        y22 = cmodel(x2)

        a = y11-y12
        b = y21-y22

        cos_dist = 0
        for a_, b_ in zip(a, b):
            for idx in range(a_.shape[0]):
                cos_dist += (1-dot(a_[idx], b_[idx])/(norm(a_[idx])*norm(b_[idx]))) / a_.shape[0]

        avg_err += cos_dist / a.shape[0]

        cnt += 1
    """

    return float(avg_err) / cnt

def evaluate(model, groups, subnets, parser, datagen, train_func, gmode=False, custom_objects=None):

    if gmode:
        lambda_ = 1.0
    else:
        lambda_ = 0.0
   
    parsers = [
        PruningNNParser(subnet, custom_objects=custom_objects) for subnet in subnets
    ]
    for p in parsers:
        p.parse()

    feats = []
    for g in groups:
        bottom = g[0][2][0][0]
        if g[0][2][0][1] > g[0][2][1][1]:
            bottom = g[0][2][1][0]
        feats.append(model.get_layer(bottom).output)
    feat_model = tf.keras.Model(model.inputs, feats)

    data_keep = []
    feat_data = [[] for _ in range(len(groups))]
    for k, data in enumerate(datagen):
        feat = feat_model(data[0])
        feat = [
            f.numpy() for f in feat
        ]
        data_keep.append(data[0])
        for gidx, f in enumerate(feat):
            feat_data[gidx].append(f)
        if k == 256:
            break
 
    curr_include = [[0] for _  in range(len(groups))]
    history = [(4, 4), (4, 10), (4, 5), (5, 6), (5, 12), (5, 10), (5, 5), (0, 8), (0, 4), (0, 10), (0, 5), (1, 4), (1, 8), (3, 4), (3, 10), (3, 5), (2, 2), (2, 5), (2, 10), (0, 0)] # vit
    history = [(0, 1), (1, 1), (4, 2), (4, 5), (3, 2), (3, 1), (2, 2), (2, 1), (2, 0), (2, 3), (3, 0), (3, 3), (4, 0), (4, 1), (4, 3), (4, 4), (4, 6), (4, 7), (1, 0), (0, 0)] # efnet
    history = [(4, 4), (3, 1), (1, 1), (4, 2), (0, 1), (4, 5), (2, 2), (3, 2), (2, 1), (2, 0), (2, 3), (3, 0), (3, 3), (4, 0), (4, 1), (4, 3), (4, 6), (4, 7), (1, 0), (0, 0)]
    history = [(5, 1), (7, 6), (7, 4), (7, 2), (7, 5), (1, 4), (0, 4), (6, 4), (3, 4), (1, 2), (0, 2), (4, 4), (2, 1), (0, 5), (6, 2), (6, 5), (1, 5), (5, 5), (5, 2), (3, 2)] #vit
    #history = [(0, 1), (1, 1), (4, 1), (2, 1), (2, 2), (5, 2), (5, 1), (3, 4), (3, 2), (3, 5)] # mbv3large
    history =  [(1, 2), (2, 2), (5, 2), (5, 5), (4, 2), (7, 1), (4, 5), (6, 1), (3, 2), (3, 5), (1, 1), (0, 1), (2, 1), (1, 0), (1, 3), (2, 0), (2, 3), (3, 0), (3, 1), (3, 3)] # efnet2
    history = [(4, 4), (4, 10), (5, 6), (5, 12), (5, 10), (0, 8), (0, 4), (0, 10), (1, 4), (1, 8), (5, 5), (4, 5), (0, 5), (3, 4), (3, 10), (3, 5), (2, 2), (2, 1), (2, 9), (1, 0)] # vit
    history = [(2, 1), (5, 4), (5, 10), (1, 2), (4, 2), (3, 1), (6, 1), (4, 1), (3, 2), (3, 0), (3, 3), (4, 0), (4, 3), (5, 0), (5, 2), (5, 6), (5, 8), (5, 12), (5, 14), (0, 1)]
    history = [(2, 1), (5, 4), (5, 10), (1, 2), (6, 1), (4, 1), (3, 1), (0, 1), (5, 5), (4, 2), (2, 2), (4, 5), (1, 1), (3, 2), (3, 5), (1, 0), (1, 3), (2, 0), (2, 3), (3, 0)]
    history = [(2, 1), (5, 4), (1, 2), (4, 2), (5, 2), (3, 2), (5, 0), (5, 6), (4, 1), (3, 4), (5, 5), (6, 1), (3, 0), (3, 6), (4, 0), (4, 3), (5, 1), (5, 3), (5, 7), (0, 1)]

    #ghistory = [ [] for _ in range(len(groups))]
    #for i, (k, p) in enumerate(history):
    #    ghistory[k].append(p)

    #ratio = 0.5
    #for k in range(len(ghistory)):
    #    for i, p in enumerate(ghistory[k]):
    #        if 0.25 < float(i) / len(ghistory[k]):
    #            break
    #        if p == 0:
    #            curr_include[k].remove(p)
    #        else:
    #            curr_include[k].append(p)

    for i, (k, p) in enumerate(history):
        print("**************", float(i) / len(history))
        if  0.45 <  float(i) / len(history):
            break
        if p == 0:
            curr_include[k].remove(p)
        else:
            curr_include[k].append(p)

    #curr_include = [[4, 0], [4, 2, 0], [1, 0, 4], [4, 0, 2], [4, 0], [1, 0], [4, 0], [6, 4, 2, 0]]
    #curr_include = [[4, 0], [4, 2, 6], [1, 5, 0], [4, 2, 6], [4, 0], [1, 0, 2], [4, 0], [6, 4, 2]]
    print("++++++", curr_include)
    psets = construct_pathset(groups, curr_include)
    print(psets)
    cmodel = psets2model(model, groups, psets, parser, custom_objects)
    err = eval_(model, cmodel, data_keep)
    print(err)

    scores = []
    cmodel_groups = [[] for _ in range(len(groups))]
    tf.keras.backend.set_floatx("float64")
    for k, (group, subnet, local_parser, feat) in enumerate(zip(groups, subnets, parsers, feat_data)):
        local_include = [curr_include[k]]
        local_groups = parse(subnet, local_parser) 
        psets = construct_pathset(local_groups, local_include)
        cmodel_ = psets2model(subnet, local_groups, psets, local_parser, custom_objects)
        cmodel_ = change_dtype(cmodel_, "float64", custom_objects=custom_objects)
        cmodel_groups[k].append(cmodel_)
    model = change_dtype(model, "float64", custom_objects=custom_objects)
    house = make_train_model(model, groups, cmodel_groups, scale=1.0, teacher_freeze=True)

    train_func(house)

    for k in range(len(cmodel_groups)):
        cmodel_ = cmodel_groups[k][0]

        for layer in cmodel_.layers:
            try:
                cmodel.get_layer(layer.name).set_weights(layer.get_weights())
                print(layer.name, " transferred...")
            except Exception as e:
                pass

    return cmodel

    curr_include = [[0] for _  in range(len(groups))]
    history = []
  
    scores = []
    local_group_info = []
    flops = []
    for k, (group, subnet, local_parser, feat) in enumerate(zip(groups, subnets, parsers, feat_data)):
        local_include = [curr_include[k]]
        local_groups = parse(subnet, local_parser) 
        local_group_info.append(local_groups)
        psets = construct_pathset(local_groups, local_include)
        cmodel = psets2model(subnet, local_groups, psets, local_parser, custom_objects)
        scores.append(eval_(subnet, cmodel, feat))

        subnet_input = list(subnet.input.shape[1:])
        print(subnet_input)
        subnet_input = tuple([1] + subnet_input)
        data = np.random.rand(*subnet_input)
        print(data.shape)
        flops.append(
            get_flops(subnet, [np.random.rand(*subnet_input)])
        )

    for i in range(20):
        best_val = None
        best_score = None
        best = None
        prev_val = None
        tf.keras.backend.clear_session()
        for k, (group, subnet, local_parser, feat, local_groups) in enumerate(zip(groups, subnets, parsers, feat_data, local_group_info)):
            for p in range(pow(2, len(group))):
                if p == 0 and len(curr_include[k]) > 1:
                    if 0 not in curr_include[k]:
                        continue
                    zero_removal = True
                elif p in curr_include[k]:
                    continue
                elif p == 0:
                    continue
                else:
                    zero_removal = False

                if zero_removal:
                    curr_include[k].remove(0)
                else:
                    curr_include[k].append(p)
                print("++++++++++", curr_include)

                local_include = [curr_include[k]]
                psets = construct_pathset(local_groups, local_include)
                cmodel = psets2model(subnet, local_groups, psets, local_parser, custom_objects)

                tf.keras.utils.plot_model(cmodel, "cmodel.pdf")
                err = eval_(subnet, cmodel, feat)

                gpsets = construct_pathset(groups, curr_include)
                gcmodel = psets2model(model, groups, gpsets, parser, custom_objects)
                err_base = eval_(model, gcmodel, data_keep)

                subnet_input = list(subnet.input.shape[1:])
                subnet_input = tuple([1] + subnet_input)
                flops_delta = float(get_flops(cmodel, [np.random.rand(*subnet_input)])) / flops[k]

                if zero_removal:
                    curr_include[k].append(0)
                else:
                    curr_include[k].pop()

                new_err = (sum(scores) - scores[k] + err) * (1-lambda_) + err_base * lambda_
                new_err = new_err * (max(flops_delta, 0.5))

                print(new_err, best_val)
                if best_val is None or best_val > new_err:
                    best_val = new_err
                    best_score = err
                    best = (k, p)

        print(prev_val, best_val)
        if prev_val is not None and prev_val <= best_val:
            print("BREAK:", prev_val, best_val)
            break

        if best[1] == 0:
            curr_include[best[0]].remove(best[1])
        else:
            curr_include[best[0]].append(best[1])
        history.append(best)
        prev_val = best_val

        scores[best[0]] = best_score
        print(scores)


    print("final:", curr_include)
    print("history:", history)

    cmodel_groups = [[] for _ in range(len(groups))]
    for k, (group, subnet, local_parser, feat, local_groups) in enumerate(zip(groups, subnets, parsers, feat_data, local_group_info)):
        local_include = [curr_include[k]]
        psets = construct_pathset(local_groups, local_include)
        cmodel = psets2model(subnet, local_groups, psets, local_parser, custom_objects)
        cmodel_groups[k].append(cmodel)
    make_train_model(model, groups, cmodel_groups, scale=1.0, teacher_freeze=True)

    psets = construct_pathset(groups, curr_include)
    cmodel = psets2model(model, groups, psets, parser, custom_objects)

    return cmodel

def parse(model, parser, model_type="efnet"):

    max_len = 4
    model_dict = json.loads(model.to_json())
    groups = []
    group = None
    for layer in model_dict["config"]["layers"]:
        if layer["class_name"] == "Add":
            left = None
            right = None
            for ib in layer["inbound_nodes"]:
                left = model.get_layer(ib[0][0])
                right = model.get_layer(ib[1][0])

                if (left.__class__.__name__ == "BatchNormalization" and right.__class__.__name__ == "Activation"): # Resnet
                    if group is None or len(group) > max_len:
                        group = []
                        groups.append(group)
                elif (left.__class__.__name__ == "Activation" and right.__class__.__name__ == "BatchNormalization"): # Resnet
                    if group is None or len(group) > max_len:
                        group = []
                        groups.append(group)
                elif (left.__class__.__name__ == "BatchNormalization" and right.__class__.__name__ == "BatchNormalization" and "resnet" in model_type): # Resnet
                    if group is not None:
                        group = None
                elif (left.__class__.__name__ != "Add" and right.__class__.__name__ != "Add") or len(group) > max_len: # start group
                    group = [] # assign new group
                    groups.append(group)

                pair = [(left.name, parser.torder[left.name]), (right.name, parser.torder[right.name])]

                assert len(ib) == 2
            assert len(layer["inbound_nodes"]) == 1
            if group is not None:
                group.append((layer["name"], parser.torder[layer["name"]], pair))
    return groups

def rewire(datagen, model, parser, train_func, gmode=True, model_type="efnet", custom_objects=None):

    model = change_dtype(model, "float32", custom_objects=custom_objects)

    tf.keras.utils.plot_model(model, "omodel.pdf", show_shapes=True)
    groups = parse(model, parser)

    subnets = []
    new_groups = []
    for i, g in enumerate(groups):
        bottom = g[0][2][0][0]
        if g[0][2][0][1] > g[0][2][1][1]:
            bottom = g[0][2][1][0]
        top = g[-1][0]
       
        layers = [
            layer for layer in model.layers if parser.torder[bottom] <= parser.torder[layer.name] and\
                parser.torder[layer.name] <= parser.torder[top]
        ]
        
        subnet, _, _ = parser.get_subnet(layers, model, custom_objects=custom_objects)

        if len(subnet.inputs) > 1:
            continue
        new_groups.append(g)

        subnet = change_dtype(subnet, "float32", custom_objects=custom_objects)

        tf.keras.utils.plot_model(subnet, "%d.pdf"%i)
        subnets.append(subnet)
    
    # groups 
    #   - group
    #       - (add_layer_name, add_layer_torder, pair) ...
    #           - pair: [(left, left_tordr), (right, right_torder)]
    cmodel = evaluate(model, new_groups, subnets, parser, datagen, train_func, gmode, custom_objects)

    tf.keras.utils.plot_model(cmodel, "model.pdf", show_shapes=True)
    tf.keras.models.save_model(cmodel, "%s_75.h5" % model_type)

    print(model.summary())
    print(cmodel.summary())

    """
    from keras_flops import get_flops
    flops = get_flops(model, batch_size=1)
    print(f"FLOPS: {flops / 10 ** 9:.06} G")

    flops = get_flops(cmodel, batch_size=1)
    print(f"FLOPS: {flops / 10 ** 9:.06} G")

    from profile import measure

    x = measure(model, "onnx_cpu")
    y = measure(cmodel, "onnx_cpu")

    print(x, y)
    """

    from profile import measure

    #x = measure(model, "onnx_cpu")
    #y = measure(cmodel, "onnx_cpu")

     
    return cmodel

def apply_rewiring(train_data_generator, teacher, gated_model, groups, l2g, parser, target_ratio, save_dir, save_prefix, save_steps, train_func, model_type="efnet", custom_objects=None):

    rewire(train_data_generator, teacher, parser, train_func, True, model_type, custom_objects)


    xxx


